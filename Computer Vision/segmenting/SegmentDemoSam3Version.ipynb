{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3681b42-fe75-4983-a0d6-7905f8e73f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Load image\n",
    "# -----------------------------\n",
    "imageloc = \"images/plant.jpg\"\n",
    "\n",
    "# Load as RGB numpy array (H, W, 3)\n",
    "image_pil = Image.open(imageloc).convert(\"RGB\")\n",
    "image = np.array(image_pil)\n",
    "\n",
    "h, w, _ = image.shape\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Center crop + HSV auto leaf point (your original logic)\n",
    "# -----------------------------\n",
    "crop_size = 1  # 1 == whole image; change to e.g. 0.2 for central 20%\n",
    "h0 = int(h * (0.5 - crop_size / 2))\n",
    "h1 = int(h * (0.5 + crop_size / 2))\n",
    "w0 = int(w * (0.5 - crop_size / 2))\n",
    "w1 = int(w * (0.5 + crop_size / 2))\n",
    "\n",
    "center_crop = image[h0:h1, w0:w1, :]\n",
    "\n",
    "center_crop_bgr = cv2.cvtColor(center_crop, cv2.COLOR_RGB2BGR)\n",
    "hsv = cv2.cvtColor(center_crop_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Typical green range in HSV (tweak as needed)\n",
    "mask_green = (\n",
    "    (hsv[..., 0] > 35) & (hsv[..., 0] < 85) &\n",
    "    (hsv[..., 1] > 50) & (hsv[..., 2] > 50)\n",
    ")\n",
    "\n",
    "candidates = np.argwhere(mask_green)\n",
    "plantloc = None\n",
    "\n",
    "if candidates.shape[0]:\n",
    "    # Find the green pixel closest to the center of the crop\n",
    "    center_y, center_x = np.array(mask_green.shape) // 2\n",
    "    dists = np.sum((candidates - [center_y, center_x]) ** 2, axis=1)\n",
    "    chosen = candidates[np.argmin(dists)]\n",
    "    y_rel, x_rel = chosen\n",
    "    y_abs = y_rel + h0\n",
    "    x_abs = x_rel + w0\n",
    "    plantloc = [[x_abs, y_abs]]\n",
    "    print(\"Auto-selected leaf point (HSV):\", plantloc)\n",
    "else:\n",
    "    print(\"No green region found in center crop.\")\n",
    "    # You can fall back to a manual point here if you want:\n",
    "    # plantloc = [[w // 2, h // 2]]\n",
    "\n",
    "# Optional: visualize HSV mask overlay for debugging\n",
    "overlay = center_crop.copy()\n",
    "overlay[mask_green] = [255, 0, 0]  # mark detected plant-ish regions in red\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(center_crop)\n",
    "# plt.title(\"Center crop\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(overlay)\n",
    "# plt.title(\"Green mask overlay\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. SAM 3: load model + processor\n",
    "# -----------------------------\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor  # name from SAM3 README\n",
    "\n",
    "# Choose device (your teammate should have CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Build SAM3 image model and processor\n",
    "# NOTE: this will internally load checkpoints from Hugging Face;\n",
    "# they must have run `hf auth login` and requested SAM3 access.\n",
    "model = build_sam3_image_model().to(device)\n",
    "processor = Sam3Processor(model)\n",
    "\n",
    "# -----------------------------\n",
    "# 3A. Use SAM3 with a TEXT prompt (\"plant leaves\")\n",
    "#     (simpler to wire; SAM3 will segment all instances of that concept)\n",
    "# -----------------------------\n",
    "inference_state = processor.set_image(image_pil)\n",
    "\n",
    "# You can tune this phrase based on your images:\n",
    "text_prompt = \"green plant leaves\"\n",
    "\n",
    "output = processor.set_text_prompt(\n",
    "    state=inference_state,\n",
    "    prompt=text_prompt,\n",
    ")\n",
    "\n",
    "# SAM3 returns instance masks + boxes + scores\n",
    "masks = output[\"masks\"]   # shape: (N, H, W) or similar\n",
    "boxes = output[\"boxes\"]   # (N, 4)\n",
    "scores = output[\"scores\"] # (N,)\n",
    "\n",
    "# -----------------------------\n",
    "# 3B. Convert masks to numpy and build union mask\n",
    "# -----------------------------\n",
    "if isinstance(masks, torch.Tensor):\n",
    "    masks_np = masks.detach().cpu().numpy()\n",
    "else:\n",
    "    masks_np = np.asarray(masks)\n",
    "\n",
    "print(\"SAM3 returned\", masks_np.shape[0], \"instance masks for prompt:\", text_prompt)\n",
    "\n",
    "# Optionally filter by score threshold\n",
    "score_thresh = 0.5\n",
    "keep = scores >= score_thresh\n",
    "if isinstance(keep, torch.Tensor):\n",
    "    keep = keep.cpu().numpy().astype(bool)\n",
    "\n",
    "if keep.sum() == 0:\n",
    "    print(f\"No masks above score {score_thresh}. \"\n",
    "          f\"Using all masks anyway.\")\n",
    "    keep[:] = True\n",
    "\n",
    "masks_kept = masks_np[keep]\n",
    "\n",
    "# Union over instances â†’ single plant mask\n",
    "union_mask = masks_kept.astype(bool).any(axis=0)  # (H, W) boolean\n",
    "\n",
    "num_segmented_pixels = int(union_mask.sum())\n",
    "print(\"Number of SAM3 plant pixels (union of instances):\", num_segmented_pixels)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Visualize original vs SAM3 segmentation\n",
    "# -----------------------------\n",
    "segmented_image = np.zeros_like(image)\n",
    "segmented_image[union_mask] = image[union_mask]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axs[0].imshow(image)\n",
    "axs[0].set_title(\"Original image\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "axs[1].imshow(segmented_image)\n",
    "axs[1].set_title(f\"SAM3 segmentation ({num_segmented_pixels} px)\")\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dad72bb-3161-40f0-9d6d-0730fb9326c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c60c2-fe7e-4fe6-9242-c423272b5a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
